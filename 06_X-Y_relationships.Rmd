# X-Y Relationships

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Un-attach any packages that happen to already be loaded. In general this is unnecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Chapter packages we will use
library(tidyverse)

# Don't show the R code!
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```


### Scatter plots
Basic idea is to build off of a scatter plot. This visualizes the relationship between two continuous variables.

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) +
  geom_point() +
  labs( x= 'Sepal Length (cm)', y='Petal Length (cm)', 
        title='Iris Sepal vs Petal Lengths')
```


In a scatter plot we can see the relationship between two variables. We can see the relationship among more variables (either continuous or discrete) by adding Size, Color, and Shape.

We could also add other categorical variables by adding faceting. With this combination we can visualize the relationship between up to 6 different variables.

```{r}
data(mtcars)
mtcars <- mtcars %>%
  mutate( am = factor(am) %>% fct_recode(manual='1', automatic='0') ) %>%
  rename( Cylinders = cyl, Power=hp, Transmission=am, Weight=wt, Displacement=disp ) 
ggplot(mtcars, aes(x=Weight, y=mpg, size=Power, color=Displacement)) +
  geom_point() +
  facet_grid(Transmission ~ Cylinders, labeller=label_both) +
  guides(size = guide_legend(reverse = TRUE)) +
  labs( x='Vehicle Weight (tons)', y='Miles per Gallon', title='Vehicle efficiency vs Size and Power')
```


### Pairs plots (All-vs-all scatterplots)

### Correlation Plots
## Pearson's Correlation Coefficient

We first consider Pearson's correlation coefficient, which is a statistics that measures the strength of the linear relationship between the predictor and response. Consider the following Pearson's correlation statistic
$$r=\frac{\sum_{i=1}^{n}\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)}{n-1}$$
where $x_{i}$ and $y_{i}$ are the x and y coordinate of the $i$th observation. Notice that each parenthesis value is the standardized value of each observation. If the x-value is big (greater than $\bar{x}$) and the y-value is large (greater than $\bar{y}$), then after multiplication, the result is positive. Likewise if the x-value is small and the y-value is small, both standardized values are negative and therefore after multiplication the result is positive. If a large x-value is paired with a small y-value, then the first value is positive, but the second is negative and so the multiplication result is negative.


```{r}
set.seed(43456)
n <- 20
data <- data.frame( x = runif(n, 0, 1) )
data <- mutate(data, y= 2 + x + rnorm(n, sd=.2))
ggplot(data, aes(x=x, y=y)) + geom_point() + geom_smooth(method='lm')
```

```{r, echo=FALSE}
xbar <- mean(data$x ) 
ybar <- mean(data$y )
tile.data <- data.frame(x=c(xbar-1, xbar-1, xbar+1, xbar+1),
                        y=c(ybar-1, ybar+1, ybar+1, ybar-1),
                        sign=c('Positive','Negative','Positive','Negative'))

ggplot(data, aes(x=x, y=y)) +
  geom_tile(data=tile.data, aes(fill=sign), alpha=.5) +
  geom_point() +
  coord_cartesian(xlim=range(data$x), ylim=range(data$y)) +
  scale_x_continuous(breaks=xbar, label=expression(bar(x))) +
  scale_y_continuous(breaks=ybar, label=expression(bar(y))) +
  labs(x=NULL, y=NULL) +
  guides(fill = guide_legend(reverse = TRUE)) 

```


The following are true about Pearson's correlation coefficient:

1. $r$ is unit-less because we have standardized the $x$ and $y$ values.
2. $-1\le r\le1$ because of the scaling by $n-1$
3. A negative $r$ denotes a negative relationship between $x$ and $y$, while a positive value of $r$ represents a positive relationship.
4. $r$ measures the strength of the *linear* relationship between the predictor and response.

```{r, echo=FALSE}
set.seed(345)
n <- 100
x <- seq(0,1, length=n) 
lots.data <- rbind(
  data.frame(x=x, y=2 + 2*x + rnorm(n, sd=.5), grp=1),
  data.frame(x=x, y=2 - 2*x + rnorm(n, sd=.5), grp=2),
  data.frame(x=x, y=2 + 2*x + rnorm(n, sd=.1), grp=3),
  data.frame(x=x, y=2 - 2*x + rnorm(n, sd=.1), grp=4),
  data.frame(x=x, y=2 - 0*x + rnorm(n, sd=2), grp=5),
  data.frame(x=x, y=2*(2*x-1)^2 + rnorm(n, sd=.1), grp=6))

R.values <-    lots.data %>% dplyr::group_by(grp) %>% dplyr::summarise(R=cor(x,y))
Plots <- list() 
for(i in 1:6){
  Plots[[i]] <- ggplot(lots.data %>% filter(grp==i), aes(x=x, y=y)) +
     geom_point() +
    ggtitle(paste('r = ', round(R.values[i,2], digits=3))) +
    labs(x=NULL, y=NULL) +
    scale_x_continuous(labels=NULL) +
    scale_y_continuous(labels=NULL) 
}  
cowplot::plot_grid(
  Plots[[1]], Plots[[2]], Plots[[3]], 
  Plots[[4]], Plots[[5]], Plots[[6]],
  ncol=3) 
```


```{r}
mtcars %>%
  select( Power, Displacement, Weight, mpg ) %>%
GGally::ggpairs(.)
```


## Overplotting

### Transparency

Wilke's book uses and example of departure time of day versus the delay amount for all the flights out of New York City in 2013. The story to take home is that longer delays tend to happen later in the afternoon or evening rather than in the morning. Wilke uses these data to argue that overplotting is annoying and that a heat map can help out.

```{r, cache=TRUE}
# Too many data points and GitHub is refusing to build the web page because this is too big.  
# I need to figure out how to save it as a png or something similar.
data('flights', package='nycflights13')
flights <- flights %>%
  # slice(1:1000) %>%
  mutate( hour = floor(dep_time / 100) ) %>%
  mutate( minute = dep_time - hour*100 ) %>%
  mutate( time = hour + minute/60 ) 

ggplot(flights, aes(x=time, y=dep_delay)) +
  # scale_y_log10( minor=c( seq(10,100,by=10), seq(100,1000,by=100) ) ) +
  scale_x_continuous(breaks=c(0, 6, 12, 18, 24),
                     labels=c('0:00', '6 am', 'noon', '6 pm', 'midnight'),
                     minor_breaks = 0:24) +
  geom_point(alpha=.1) +
  labs( x='Time of Day', y='Departure Delay', title='2013 Airline Departure Delays')

```


### Intensity Maps
No matter how much we adjust the transparency, we can't really fix this because there is so much data. If for each area on the graph, we count how many observations fall into the region, we can color the area based on how many observations are in the region.

```{r, cache=TRUE}
ggplot(flights, aes(x=time, y=dep_delay)) +
  scale_x_continuous(breaks=c(0, 6, 12, 18, 24),
                     labels=c('0:00', '6 am', 'noon', '6 pm', 'midnight'),
                     minor_breaks = 0:24) +
  # scale_y_log10( minor=c( seq(10,100,by=10), seq(100,1000,by=100) ) ) +
  geom_hex(bins=80) +
  labs( x='Time of Day', y='Departure Delay', title='2013 Airline Departure Delays')
```

This graph leads me to think that MOST flights are quite late, when if fact, they aren't. This is due to the problem of "proportional pixels". There is so much space and color devoted to flights that are more than 30 minutes late that the viewer can't help but have that impression.

```{r, cache=TRUE}
flights %>%
  mutate(delay = cut(dep_delay, breaks=c(-30,-10,0,10,30,60,120,180,Inf)) ) %>%
  select(delay) %>% drop_na() %>% group_by(delay) %>%
  count() %>% ungroup() %>%
  mutate( proportion = n/sum(n) ) %>%
  pander::pander()
```

Because we are interest in the time distribution of significant delays, and early departures are usually only by a couple of minutes, we'll we'll take a log$_{10}$ transformation of all the delays greater than 10 minutes.

```{r, cache=TRUE}
flights %>%
  filter( dep_delay > 10 )%>%
  ggplot(., aes(x=time, y=dep_delay)) +
  scale_x_continuous(breaks=c(0, 6, 12, 18, 24),
                     labels=c('0:00', '6 am', 'noon', '6 pm', 'midnight'),
                     minor_breaks = 0:24) +
  scale_y_log10( minor=c( seq(10,100,by=10), seq(100,1000,by=100) ) ) +
  scale_fill_gradient(low='white', high='blue') +
  geom_hex(bins=80) +
  labs( x='Time of Day', y='Departure Delay', 
        title='2013 Airline Departure Delays longer than 10 minutes')
```


### Contour Plots

```{r, cache=TRUE}
flights %>%
  filter( dep_delay > 10 )%>%
  ggplot(., aes(x=time, y=dep_delay)) +
  scale_x_continuous(breaks=c(0, 6, 12, 18, 24),
                     labels=c('0:00', '6 am', 'noon', '6 pm', 'midnight'),
                     minor_breaks = 0:24) +
  scale_y_log10(  ) +
  scale_fill_gradient(low='white', high='blue') +
  geom_density_2d() +
  labs( x='Time of Day', y='Departure Delay', 
        title='2013 Airline Departure Delays longer than 10 minutes')
```


## Exercises

1. Read Chapters 12 and 18 from Wilke's book. Feel free to skip section 12.3.

2. We will use a smaller version of the diamonds dataset that Wilke uses in his Chapter 18. You can download it from my GitHub site in a 
[.csv file](https://raw.githubusercontent.com/dereksonderegger/141/master/data-raw/Small_Diamonds.csv). 
We will examine price, carats, cut (Fair, Good, Very Good, Premium, and Ideal), color (J - D), and clarity (I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF), where the category labels I've given are from worst to best. 
    a) Had I not told you that clarity level IF is the best clarity, what graphs could you make to figure that out? Create and show a graph that demonstrates this and explain your graph. 
    b) If the diamond clarity isn't good, the dimond cutter won't worry too much about the quality of cut. Make a graph that demonstrates that and explain your reasoning.
    c) While in principle it is possible for the dimond carats to be any number, they are often cut to be some common carat size. Create a visualization that shows this and discuss how the carat size changes as the cut and clarity improve.
    
3. From the [Gapminder.com](www.gapminder.com/data) website, I've downloaded a bunch of interesting covariates about countries. You can find my dataset at my GitHub site in a 
[.csv file](https://raw.githubusercontent.com/dereksonderegger/141/master/data-raw/Gapminder.csv). 
The variables I've included include the country region, year, population size, population growth, percent of population with basic sanitation, GDP per capita, Total GDP, life expectancy, adult male and female labor force participation rates. *Fertility is the number of children per woman, so a fertility rate of 2 children per woman is a stable population.* 
    a) For all the following questions, only consider the year 2015. 
    b) Investigate the relationship between GDP and GDP_per_capita. Why should we prefer to work with GDP_per_capita when comparing standards of living between, say, the United States and Canada?
    c) Investigate the relationship between life expectancy, fertility, and GDP_per_capita. Do these relationships seem to vary by region?  Comment on your graphs and relationships that you observe.
    d) Investigate the relationship relationship between life expectancy, adult female labor force participation and fertility. Does this vary by region? Comment on your graphs and relationships that you observe.
    


